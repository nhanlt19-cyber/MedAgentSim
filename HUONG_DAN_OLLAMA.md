# H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Model Ollama v·ªõi MedAgentSim

## Model: Llama-3.3-8B-Instruct-128K.Q4_K_M

Model n√†y **ho√†n to√†n ph√π h·ª£p** v√† c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c v·ªõi MedAgentSim. ƒê√¢y l√†:
- **Model**: Llama 3.3 8B Instruct
- **Context**: 128K tokens
- **Quantization**: Q4_K_M (4-bit quantization, medium quality)
- **Format**: GGUF (Ollama format)

## Y√™u C·∫ßu

1. **Ollama ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t** tr√™n server Ubuntu
2. **Model ƒë√£ ƒë∆∞·ª£c pull** v·ªÅ m√°y

## Ki·ªÉm Tra Model

### 1. Ki·ªÉm tra Ollama ƒë√£ c√†i ƒë·∫∑t

```bash
ollama --version
```

### 2. Ki·ªÉm tra model ƒë√£ ƒë∆∞·ª£c pull

```bash
ollama list
```

N·∫øu model ch∆∞a c√≥, pull model:

```bash
ollama pull Llama-3.3-8B-Instruct-128K.Q4_K_M
```

**L∆∞u √Ω**: T√™n model trong Ollama c√≥ th·ªÉ kh√°c m·ªôt ch√∫t. Ki·ªÉm tra t√™n ch√≠nh x√°c:

```bash
# Xem t·∫•t c·∫£ models
ollama list

# Ho·∫∑c t√¨m model Llama 3.3
ollama list | grep -i llama
```

T√™n model c√≥ th·ªÉ l√†:
- `llama3.3:8b`
- `llama3.3:8b-instruct`
- `Llama-3.3-8B-Instruct-128K.Q4_K_M`
- ho·∫∑c t√™n kh√°c t√πy v√†o c√°ch b·∫°n pull

### 3. Test model

```bash
ollama run Llama-3.3-8B-Instruct-128K.Q4_K_M "Hello, how are you?"
```

---

## C√°ch S·ª≠ D·ª•ng v·ªõi MedAgentSim

### Ph∆∞∆°ng Ph√°p 1: S·ª≠ d·ª•ng tham s·ªë Ollama (Khuy·∫øn ngh·ªã)

```bash
# Terminal 1: Ch·∫°y server
conda activate mgent
python -m medsim.server

# Terminal 2: Ch·∫°y simulation v·ªõi Ollama
conda activate mgent
python medsim/main.py \
  --inf_type llm \
  --doctor_bias None \
  --patient_bias None \
  --doctor_llm meta-llama/Llama-3.3-8B-Instruct \
  --patient_llm meta-llama/Llama-3.3-8B-Instruct \
  --measurement_llm meta-llama/Llama-3.3-8B-Instruct \
  --moderator_llm meta-llama/Llama-3.3-8B-Instruct \
  --agent_dataset MedQA \
  --doctor_image_request False \
  --num_scenarios 10 \
  --total_inferences 20 \
  --ollama_url http://localhost:11434 \
  --ollama_model Llama-3.3-8B-Instruct-128K.Q4_K_M
```

**L∆∞u √Ω**: Thay `Llama-3.3-8B-Instruct-128K.Q4_K_M` b·∫±ng t√™n model ch√≠nh x√°c trong Ollama c·ªßa b·∫°n.

### Ph∆∞∆°ng Ph√°p 2: S·ª≠ d·ª•ng format "ollama:model_name"

N·∫øu model name trong Ollama l√† `llama3.3:8b`, b·∫°n c√≥ th·ªÉ d√πng:

```bash
python medsim/main.py \
  --doctor_llm ollama:llama3.3:8b \
  --patient_llm ollama:llama3.3:8b \
  --measurement_llm ollama:llama3.3:8b \
  --moderator_llm ollama:llama3.3:8b \
  --agent_dataset MedQA
```

### Ph∆∞∆°ng Ph√°p 3: S·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng

```bash
export OLLAMA_HOST="http://localhost:11434"
export OLLAMA_MODEL="Llama-3.3-8B-Instruct-128K.Q4_K_M"

python medsim/main.py \
  --doctor_llm ollama:${OLLAMA_MODEL} \
  --patient_llm ollama:${OLLAMA_MODEL} \
  --measurement_llm ollama:${OLLAMA_MODEL} \
  --moderator_llm ollama:${OLLAMA_MODEL} \
  --agent_dataset MedQA
```

---

## X√°c ƒê·ªãnh T√™n Model Ch√≠nh X√°c

### C√°ch 1: D√πng l·ªánh ollama list

```bash
ollama list
```

Output v√≠ d·ª•:
```
NAME                                    ID              SIZE    MODIFIED
llama3.3:8b                            7f4a2b3c4d5e    4.7 GB  2 days ago
Llama-3.3-8B-Instruct-128K.Q4_K_M      a1b2c3d4e5f6    4.8 GB  1 day ago
```

S·ª≠ d·ª•ng t√™n trong c·ªôt `NAME`.

### C√°ch 2: Test v·ªõi Python

```python
import requests

# Ki·ªÉm tra models c√≥ s·∫µn
response = requests.get("http://localhost:11434/api/tags")
models = response.json()
print("Available models:")
for model in models.get("models", []):
    print(f"  - {model['name']}")
```

### C√°ch 3: Test tr·ª±c ti·∫øp

```bash
# Test v·ªõi t√™n model
curl http://localhost:11434/api/generate -d '{
  "model": "Llama-3.3-8B-Instruct-128K.Q4_K_M",
  "prompt": "Hello"
}'
```

N·∫øu th√†nh c√¥ng, model name ƒë√∫ng. N·∫øu l·ªói, th·ª≠ t√™n kh√°c.

---

## C·∫•u H√¨nh Ollama Server T·ª´ Xa (N·∫øu C·∫ßn)

N·∫øu Ollama ch·∫°y tr√™n server kh√°c (kh√¥ng ph·∫£i localhost):

### 1. C·∫•u h√¨nh Ollama server

Tr√™n server ch·∫°y Ollama, ch·ªânh s·ª≠a `/etc/systemd/system/ollama.service`:

```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

Sau ƒë√≥ restart:
```bash
sudo systemctl restart ollama
```

### 2. S·ª≠ d·ª•ng trong MedAgentSim

```bash
python medsim/main.py \
  --ollama_url http://10.0.12.81:11434 \
  --ollama_model Llama-3.3-8B-Instruct-128K.Q4_K_M \
  --doctor_llm meta-llama/Llama-3.3-8B-Instruct \
  --patient_llm meta-llama/Llama-3.3-8B-Instruct \
  --measurement_llm meta-llama/Llama-3.3-8B-Instruct \
  --moderator_llm meta-llama/Llama-3.3-8B-Instruct \
  --agent_dataset MedQA
```

---

## So S√°nh v·ªõi Custom LLM Server

| T√≠nh nƒÉng | Ollama | Custom LLM Server |
|-----------|--------|-------------------|
| C√†i ƒë·∫∑t | C·∫ßn c√†i Ollama | Ch·ªâ c·∫ßn URL v√† API key |
| Model | Ph·∫£i pull v·ªÅ m√°y | S·ª≠ d·ª•ng t·ª´ xa |
| T·ªëc ƒë·ªô | Nhanh (local) | Ph·ª• thu·ªôc network |
| T√†i nguy√™n | C·∫ßn GPU/RAM | Kh√¥ng c·∫ßn |
| Ph√π h·ª£p | Development/Testing | Production |

---

## Troubleshooting

### L·ªói: Model not found

**Nguy√™n nh√¢n**: T√™n model kh√¥ng ƒë√∫ng ho·∫∑c ch∆∞a ƒë∆∞·ª£c pull.

**Gi·∫£i ph√°p**:
```bash
# Ki·ªÉm tra model
ollama list

# Pull model n·∫øu ch∆∞a c√≥
ollama pull llama3.3:8b
# ho·∫∑c
ollama pull Llama-3.3-8B-Instruct-128K.Q4_K_M
```

### L·ªói: Connection refused

**Nguy√™n nh√¢n**: Ollama server ch∆∞a ch·∫°y.

**Gi·∫£i ph√°p**:
```bash
# Ki·ªÉm tra Ollama ƒëang ch·∫°y
ps aux | grep ollama

# Kh·ªüi ƒë·ªông Ollama
ollama serve
# ho·∫∑c
sudo systemctl start ollama
```

### L·ªói: Timeout

**Nguy√™n nh√¢n**: Model qu√° l·ªõn ho·∫∑c thi·∫øu RAM/GPU.

**Gi·∫£i ph√°p**:
- S·ª≠ d·ª•ng model nh·ªè h∆°n (Q4 thay v√¨ Q8)
- TƒÉng timeout trong code
- ƒê·∫£m b·∫£o ƒë·ªß RAM/VRAM

---

## T·ªëi ∆Øu Hi·ªáu Su·∫•t

### 1. S·ª≠ d·ª•ng GPU (n·∫øu c√≥)

Ollama t·ª± ƒë·ªông s·ª≠ d·ª•ng GPU n·∫øu c√≥ CUDA. Ki·ªÉm tra:

```bash
ollama ps
```

### 2. Gi·∫£m context length (n·∫øu c·∫ßn)

Model 128K context r·∫•t l·ªõn. N·∫øu kh√¥ng c·∫ßn, d√πng model nh·ªè h∆°n:

```bash
ollama pull llama3.3:8b  # Context nh·ªè h∆°n
```

### 3. T·ªëi ∆∞u quantization

- **Q4_K_M**: C√¢n b·∫±ng t·ªët (khuy·∫øn ngh·ªã)
- **Q4_0**: Nh·ªè h∆°n, nhanh h∆°n, ch·∫•t l∆∞·ª£ng th·∫•p h∆°n
- **Q8_0**: L·ªõn h∆°n, ch·∫≠m h∆°n, ch·∫•t l∆∞·ª£ng cao h∆°n

---

## V√≠ D·ª• Ho√†n Ch·ªânh

```bash
# 1. ƒê·∫£m b·∫£o Ollama ƒëang ch·∫°y
ollama serve &

# 2. Ki·ªÉm tra model
ollama list | grep -i llama

# 3. N·∫øu ch∆∞a c√≥, pull model
ollama pull llama3.3:8b

# 4. Test model
ollama run llama3.3:8b "Test"

# 5. Ch·∫°y MedAgentSim
conda activate mgent

# Terminal 1
python -m medsim.server

# Terminal 2
python medsim/main.py \
  --inf_type llm \
  --doctor_llm meta-llama/Llama-3.3-8B-Instruct \
  --patient_llm meta-llama/Llama-3.3-8B-Instruct \
  --measurement_llm meta-llama/Llama-3.3-8B-Instruct \
  --moderator_llm meta-llama/Llama-3.3-8B-Instruct \
  --agent_dataset MedQA \
  --num_scenarios 5 \
  --total_inferences 20 \
  --ollama_url http://localhost:11434 \
  --ollama_model llama3.3:8b
```

---

## K·∫øt Lu·∫≠n

Model **Llama-3.3-8B-Instruct-128K.Q4_K_M** ho√†n to√†n ph√π h·ª£p v√† c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c v·ªõi MedAgentSim. Ch·ªâ c·∫ßn:

1. ‚úÖ ƒê·∫£m b·∫£o Ollama ƒë√£ c√†i ƒë·∫∑t
2. ‚úÖ Pull model v·ªÅ m√°y
3. ‚úÖ X√°c ƒë·ªãnh t√™n model ch√≠nh x√°c
4. ‚úÖ S·ª≠ d·ª•ng `--ollama_url` v√† `--ollama_model` khi ch·∫°y

Ch√∫c b·∫°n th√†nh c√¥ng! üöÄ

