# HÆ°á»›ng Dáº«n Sá»­a Lá»—i: Ollama 404 Not Found

## NguyÃªn NhÃ¢n

Lá»—i `404 Client Error: Not Found for url: http://localhost:11434/api/chat` xáº£y ra khi:
1. **Ollama server khÃ´ng cháº¡y** - Server chÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng
2. **Code tá»± Ä‘á»™ng fallback sang Ollama** - Máº·c dÃ¹ báº¡n Ä‘Ã£ chá»‰ Ä‘á»‹nh LLM server tá»« xa, code váº«n cá»‘ dÃ¹ng Ollama
3. **Logic check server khÃ´ng Ä‘Ãºng** - Code check Ollama trÆ°á»›c khi dÃ¹ng custom server URL

## Giáº£i PhÃ¡p

---

## PhÆ°Æ¡ng PhÃ¡p 1: Äáº£m Báº£o DÃ¹ng LLM Server Tá»« Xa (Khuyáº¿n nghá»‹)

### Khi cháº¡y simulation, Ä‘áº£m báº£o truyá»n Ä‘Ãºng tham sá»‘:

```bash
python -m medsim.simulate \
  --doctor_llm meta-llama/Llama-3.2-3B-Instruct \
  --patient_llm meta-llama/Llama-3.2-3B-Instruct \
  --measurement_llm meta-llama/Llama-3.2-3B-Instruct \
  --moderator_llm meta-llama/Llama-3.2-3B-Instruct \
  --llm_server_url https://llmapi.iec-uit.com/v1/chat/completions \
  --llm_api_key sk-llmiec-e90a0e08c8640e7c5995037551a19af5
```

**LÆ°u Ã½:** Code Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»ƒ Æ°u tiÃªn custom server URL. Náº¿u truyá»n `--llm_server_url`, code sáº½ khÃ´ng check Ollama.

---

## PhÆ°Æ¡ng PhÃ¡p 2: Táº¯t Ollama Check (Náº¿u khÃ´ng dÃ¹ng)

### Set environment variable:

```bash
export OLLAMA_HOST=""
export DISABLE_OLLAMA=1
```

### Hoáº·c sá»­a code Ä‘á»ƒ disable Ollama:

Trong `medsim/query_model.py`, Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»ƒ khÃ´ng check Ollama náº¿u cÃ³ custom server URL.

---

## PhÆ°Æ¡ng PhÃ¡p 3: CÃ i Äáº·t vÃ  Cháº¡y Ollama (Náº¿u muá»‘n dÃ¹ng)

### Náº¿u báº¡n muá»‘n dÃ¹ng Ollama:

```bash
# CÃ i Ä‘áº·t Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Khá»Ÿi Ä‘á»™ng Ollama
ollama serve

# Pull model
ollama pull llama3.3:70b

# Test
ollama run llama3.3:70b "Hello"
```

---

## PhÆ°Æ¡ng PhÃ¡p 4: Kiá»ƒm Tra Logic Backend Selection

### Code sáº½ chá»n backend theo thá»© tá»±:

1. **Custom LLM Server** (náº¿u cÃ³ `--llm_server_url`) - âœ… Æ¯u tiÃªn cao nháº¥t
2. **Ollama** (náº¿u cÃ³ vÃ  khÃ´ng cÃ³ custom server) - âš ï¸ Chá»‰ khi khÃ´ng cÃ³ custom server
3. **Local Model** (náº¿u khÃ´ng cÃ³ server nÃ o)

### Kiá»ƒm tra backend Ä‘ang dÃ¹ng:

```bash
# Xem log khi cháº¡y simulation
# Sáº½ tháº¥y: "Using custom LLM server: https://..."
# hoáº·c: "Using Ollama server: ..."
# hoáº·c: "No server available, loading model locally..."
```

---

## Troubleshooting

### Lá»—i: Váº«n dÃ¹ng Ollama dÃ¹ Ä‘Ã£ truyá»n --llm_server_url

**NguyÃªn nhÃ¢n:** Code chÆ°a Ä‘Æ°á»£c update hoáº·c tham sá»‘ khÃ´ng Ä‘Æ°á»£c truyá»n Ä‘Ãºng.

**Giáº£i phÃ¡p:**
```bash
# Kiá»ƒm tra code Ä‘Ã£ Ä‘Æ°á»£c sync chÆ°a
cd /root/MedAgentSim
git pull

# Hoáº·c sync tá»« laptop
bash sync-to-server.sh

# Kiá»ƒm tra tham sá»‘
python -m medsim.simulate --help | grep llm_server_url
```

### Lá»—i: Ollama 404 nhÆ°ng váº«n cá»‘ dÃ¹ng

**Giáº£i phÃ¡p:**
```bash
# Táº¯t Ollama check báº±ng cÃ¡ch khÃ´ng truyá»n ollama_url
# Chá»‰ truyá»n llm_server_url
python -m medsim.simulate \
  --llm_server_url https://llmapi.iec-uit.com/v1/chat/completions \
  --llm_api_key sk-llmiec-e90a0e08c8640e7c5995037551a19af5 \
  ...
```

---

## Quick Fix

### Äáº£m báº£o dÃ¹ng LLM server tá»« xa:

```bash
# Set environment variables
export LLM_SERVER_URL="https://llmapi.iec-uit.com/v1/chat/completions"
export LLM_API_KEY="sk-llmiec-e90a0e08c8640e7c5995037551a19af5"

# Cháº¡y simulation
conda activate mgent
python -m medsim.simulate \
  --llm_server_url $LLM_SERVER_URL \
  --llm_api_key $LLM_API_KEY \
  --doctor_llm meta-llama/Llama-3.2-3B-Instruct \
  --patient_llm meta-llama/Llama-3.2-3B-Instruct \
  --measurement_llm meta-llama/Llama-3.2-3B-Instruct \
  --moderator_llm meta-llama/Llama-3.2-3B-Instruct
```

---

## LÆ°u Ã

1. **Code Ä‘Ã£ Ä‘Æ°á»£c sá»­a** Ä‘á»ƒ Æ°u tiÃªn custom server URL
2. **Náº¿u truyá»n `--llm_server_url`**, code sáº½ khÃ´ng check Ollama
3. **Náº¿u khÃ´ng truyá»n `--llm_server_url`**, code sáº½ check Ollama (náº¿u cÃ³)
4. **Äáº£m báº£o sync code má»›i nháº¥t** tá»« laptop lÃªn server

---

**Sau khi fix, cháº¡y láº¡i simulation vá»›i `--llm_server_url`!** ğŸš€

